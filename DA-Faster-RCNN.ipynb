{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXhEjILxLg2f"
      },
      "source": [
        "#Detectron2 Installation <br>\n",
        "1) Select the GPU as hardware in the runtime<br>\n",
        "2) Run the following cell.<br>\n",
        "NB: The runtime will be restarted at the end of this installation which requires few minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4Pml2azLOp4",
        "outputId": "a005c77f-84f5-4889-8a5d-9a4cd5a6ba05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-6z54jtrt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-6z54jtrt\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 8c4a333ceb8df05348759443d0206302485890e0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.13.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black (from detectron2==0.6)\n",
            "  Downloading black-23.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.41.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=6112421 sha256=ca07d2f63b54334fe248b2aa6623d0a1f45770861793944db66c98c1e2f5f509\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ph3y53pl/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=431a8367dc66ded7ec887b659c98a7e364a6cf26b369c50111d1ca5e26ae7e7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=fb17af18d33274e00c8f3cfcda31e56a5cb0ad039fb6650393c1262740b7b18a\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-23.9.1 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.11.2 portalocker-2.8.2 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "exit(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_wvanUINz2F"
      },
      "source": [
        "# Check the installation\n",
        "The right output running the following cell should be:<br><br>\n",
        "nvcc: NVIDIA (R) Cuda compiler driver<br>\n",
        "Copyright (c) 2005-2020 NVIDIA Corporation<br>\n",
        "Built on Wed_Sep_21_10:33:58_PDT_2022<br>\n",
        "Cuda compilation tools, release 11.8, V11.8.89<br>\n",
        "Build cuda_11.8.r11.8/compiler.31833905_0<br>\n",
        "torch:  2.0 ; cuda:  cu118<br>\n",
        "detectron2: 0.6<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yTjBgbKL2i4",
        "outputId": "bd731c20-815c-40e1-f98b-306108e1cdb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "torch:  2.0 ; cuda:  cu118\n",
            "detectron2: 0.6\n"
          ]
        }
      ],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eWXy_PdO7bT"
      },
      "source": [
        "#Environment Setup\n",
        "\n",
        "Replace at the following path ../usr/local/lib/python3.10/dist-packages/detectron2/modeling/meta_arch/ the **rcnn.py** script with my **rcnn.py**.<br>\n",
        "Do the same for:<br> \n",
        "**roi_heads.py** file at the path ../usr/local/lib/python3.10/dist-packages/detectron2/modeling/roi_heads/ <br>\n",
        "**rpn.py** file at the path ../usr/local/lib/python3.10/dist-packages/detectron2/modeling/proposal_generator/ <br>\n",
        "\n",
        "Inside the folder ../usr/local/lib/python3.10/dist-packages/detectron2/modeling/ create a folder called **da_modules** and upload the four files:<br>\n",
        "grad_rev_layer.py<br>\n",
        "image_level_discriminators.py<br>\n",
        "instance_level_discriminators.py<br>\n",
        "consistency_regularization_loss.py<br>\n",
        "Restart the runtime<br>\n",
        "Load the dataset in Google Drive and import it running the cell below.\n",
        "\n",
        "NB: python version can change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD7LKIwXOT5n",
        "outputId": "52a5ae09-1efc-4b76-de94-49a5fe0cb51e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "import logging\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer\n",
        "from detectron2.data import MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.solver import build_lr_scheduler, build_optimizer\n",
        "from detectron2.utils.events import EventStorage\n",
        "from detectron2.engine import default_argument_parser, default_setup, default_writers, launch\n",
        "import torch, torchvision\n",
        "from detectron2.data.datasets import register_coco_instances, load_coco_json, register_pascal_voc\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VamGIsg3T5q1"
      },
      "source": [
        "#Register your Dataset\n",
        "Run the following cell according to your dataset path. <br>\n",
        "If your annotations are in PASCAL VOC use **register_pascal_voc** otherwise **register_coco_instances** if they are in COCO format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80n1N7hkQ2xO"
      },
      "outputs": [],
      "source": [
        "register_pascal_voc(\"city_trainS\", \"drive/My Drive/cityscape/\", \"train_s\", 2007, ['car','person','rider','truck','bus','train','motorcycle','bicycle'])\n",
        "register_pascal_voc(\"city_trainT\", \"drive/My Drive/cityscape/\", \"train_t\", 2007, ['car','person','rider','truck','bus','train','motorcycle','bicycle'])\n",
        "\n",
        "register_pascal_voc(\"city_testT\", \"drive/My Drive/cityscape/\", \"test_t\", 2007, ['car','person','rider','truck','bus','train','motorcycle','bicycle'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isDeFPTOU9GE"
      },
      "outputs": [],
      "source": [
        "#This block is an example of how you should use this API with COCO annotations\n",
        "register_coco_instances(\"dataset_train_synthetic\", {}, \"drive/My Drive/Bellomo_Dataset_UDA/synthetic/Object_annotations/Training_annotations.json\", \"./drive/My Drive/Bellomo_Dataset_UDA/synthetic/images\")\n",
        "register_coco_instances(\"dataset_train_real\", {}, \"drive/My Drive/Bellomo_Dataset_UDA/real_hololens/training/training_set.json\", \"./drive/My Drive/Bellomo_Dataset_UDA/real_hololens/training\")\n",
        "\n",
        "register_coco_instances(\"dataset_test_real\", {}, \"drive/My Drive/Bellomo_Dataset_UDA/real_hololens/test/test_set.json\", \"./drive/My Drive/Bellomo_Dataset_UDA/real_hololens/test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1GhaNx9Ur0i"
      },
      "source": [
        "#Training Loop Definition\n",
        "Run the following block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87nlpSjMTQJD"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(\"detectron2\")\n",
        "\n",
        "def do_train(cfg_source, cfg_target, model, resume = False):\n",
        "\n",
        "    model.train()\n",
        "    optimizer = build_optimizer(cfg_source, model)\n",
        "    scheduler = build_lr_scheduler(cfg_source, optimizer)\n",
        "    checkpointer = DetectionCheckpointer(model, cfg_source.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler)\n",
        "\n",
        "    start_iter = (checkpointer.resume_or_load(cfg_source.MODEL.WEIGHTS, resume=resume).get(\"iteration\", -1) + 1)\n",
        "    max_iter = cfg_source.SOLVER.MAX_ITER\n",
        "\n",
        "    periodic_checkpointer = PeriodicCheckpointer(checkpointer, cfg_source.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter)\n",
        "    writers = default_writers(cfg_source.OUTPUT_DIR, max_iter) if comm.is_main_process() else []\n",
        "\n",
        "    data_loader_source = build_detection_train_loader(cfg_source)\n",
        "    data_loader_target = build_detection_train_loader(cfg_target)\n",
        "    logger.info(\"Starting training from iteration {}\".format(start_iter))\n",
        "\n",
        "    lambda_hyper = 0.1\n",
        "\n",
        "    with EventStorage(start_iter) as storage:\n",
        "        for data_source, data_target, iteration in zip(data_loader_source, data_loader_target, range(start_iter, max_iter)):\n",
        "            storage.iter = iteration\n",
        "\n",
        "            loss_dict = model(data_source, False, 1)\n",
        "            loss_dict_target = model(data_target, True, 1)\n",
        "            \n",
        "            loss_dict[\"loss_image_d\"] += loss_dict_target[\"loss_image_d\"]\n",
        "            loss_dict[\"loss_instance_d\"] += loss_dict_target[\"loss_instance_d\"]\n",
        "            loss_dict[\"loss_consistency_d\"] += loss_dict_target[\"loss_consistency_d\"]\n",
        "\n",
        "            loss_dict[\"loss_image_d\"] *= ( 0.5 * lambda_hyper)\n",
        "            loss_dict[\"loss_instance_d\"] *= ( 0.5 * lambda_hyper)\n",
        "            loss_dict[\"loss_consistency_d\"] *= ( 0.5 * lambda_hyper)\n",
        "\n",
        "            losses = sum(loss_dict.values())\n",
        "            assert torch.isfinite(losses).all(), loss_dict\n",
        "\n",
        "            loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n",
        "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "            if comm.is_main_process():\n",
        "                storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False)\n",
        "            scheduler.step()\n",
        "\n",
        "            if iteration - start_iter > 5 and ((iteration + 1) % 50 == 0 or iteration == max_iter - 1):\n",
        "                for writer in writers:\n",
        "                    writer.write()\n",
        "            periodic_checkpointer.step(iteration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdXrpSs-ZlUl"
      },
      "source": [
        "#Configuration Definition\n",
        "Define the configuration for the source (cfg_source) and target dataset (cfg_target). The cfg_source contains also the parameters which will be used by the network such us:<br>\n",
        "learning rate, number of training iterations, weight decay, number of classes etc...\n",
        "\n",
        "## Backbone for Faster RCNN\n",
        "this implementations works with three kind of backbone:<br> FPN: \"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"<br>\n",
        "DC5: \"COCO-Detection/faster_rcnn_R_50_DC5_1x.yaml\"<br>\n",
        "C4: \"COCO-Detection/faster_rcnn_R_50_C4_1x.yaml\"<br>\n",
        "\n",
        "You can also use their variants such us faster_rcnn_R_101_C4_3x, faster_rcnn_R_50_DC5_3x, faster_rcnn_R_101_DC5_3x, etc...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeF7TnM9bJSa",
        "outputId": "fd996266-34ef-4682-a1da-d1e61ea6b5dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "build_resnet_fpn_backbone\n"
          ]
        }
      ],
      "source": [
        "cfg_source = get_cfg()\n",
        "cfg_source.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
        "cfg_source.DATASETS.TRAIN = (\"city_trainS\",)\n",
        "cfg_source.DATALOADER.NUM_WORKERS = 2\n",
        "cfg_source.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\")\n",
        "cfg_source.SOLVER.IMS_PER_BATCH = 4\n",
        "cfg_source.SOLVER.BASE_LR = 0.0005\n",
        "cfg_source.SOLVER.WARMUP_FACTOR = 1.0 / 100\n",
        "cfg_source.SOLVER.WARMUP_ITERS = 1000\n",
        "cfg_source.SOLVER.MAX_ITER = 5000\n",
        "cfg_source.INPUT.MIN_SIZE_TRAIN = (600,)\n",
        "cfg_source.INPUT.MIN_SIZE_TEST = 0\n",
        "os.makedirs(cfg_source.OUTPUT_DIR, exist_ok=True)\n",
        "cfg_source.MODEL.ROI_HEADS.NUM_CLASSES = 8\n",
        "model = build_model(cfg_source)\n",
        "\n",
        "cfg_target = get_cfg()\n",
        "cfg_target.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
        "cfg_target.DATASETS.TRAIN = (\"city_trainT\",)\n",
        "cfg_target.INPUT.MIN_SIZE_TRAIN = (600,)\n",
        "cfg_target.DATALOADER.NUM_WORKERS = 0\n",
        "cfg_target.SOLVER.IMS_PER_BATCH = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTdazgytwuYH"
      },
      "source": [
        "Large datasets can require much time to be loaded from google drive. Please be patient and restart the runtime if the training does not start in few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR7zSjVJdVIO",
        "outputId": "619c3c59-7034-48a2-aff5-df9e6e3af2a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING [09/17 21:21:54 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "[09/17 21:21:54 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_1x/137257794/model_final_b275ba.pkl ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (9, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (9,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (32, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (32,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "discriminator.reducer.0.weight\n",
            "discriminator.reducer.2.weight\n",
            "discriminator.reducer.5.weight\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n",
            "roi_heads.discriminatorProposal.reducer.0.weight\n",
            "roi_heads.discriminatorProposal.reducer.2.weight\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[09/17 21:22:02 d2.data.build]: Removed 10 images with no usable annotations. 2965 images left.\n",
            "[09/17 21:22:03 d2.data.build]: Distribution of instances among all 8 categories:\n",
            "|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|    car     | 27155        |   person   | 17994        |   rider    | 1807         |\n",
            "|   truck    | 489          |    bus     | 385          |   train    | 171          |\n",
            "| motorcycle | 739          |  bicycle   | 3729         |            |              |\n",
            "|   total    | 52469        |            |              |            |              |\n",
            "[09/17 21:22:03 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(600,), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[09/17 21:22:03 d2.data.build]: Using training sampler TrainingSampler\n",
            "[09/17 21:22:03 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[09/17 21:22:03 d2.data.common]: Serializing 2965 elements to byte tensors and concatenating them all ...\n",
            "[09/17 21:22:03 d2.data.common]: Serialized dataset takes 4.09 MiB\n",
            "[09/17 21:22:09 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(600,), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[09/17 21:22:09 d2.data.build]: Using training sampler TrainingSampler\n",
            "[09/17 21:22:09 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[09/17 21:22:09 d2.data.common]: Serializing 2975 elements to byte tensors and concatenating them all ...\n",
            "[09/17 21:22:10 d2.data.common]: Serialized dataset takes 4.32 MiB\n",
            "[09/17 21:22:10 detectron2]: Starting training from iteration 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[09/17 21:24:07 d2.utils.events]:  iter: 49  total_loss: 1.932  loss_cls: 0.7193  loss_box_reg: 0.7616  loss_rpn_cls: 0.0591  loss_rpn_loc: 0.19  loss_image_d: 0.0866  loss_instance_d: 0.08039     lr: 0.0004902  max_mem: 5695M\n",
            "[09/17 21:26:01 d2.utils.events]:  eta: 1:49:29  iter: 99  total_loss: 1.557  loss_cls: 0.4791  loss_box_reg: 0.7046  loss_rpn_cls: 0.05925  loss_rpn_loc: 0.1825  loss_image_d: 0.08659  loss_instance_d: 0.02086     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:27:50 d2.utils.events]:  eta: 1:43:25  iter: 149  total_loss: 1.246  loss_cls: 0.3636  loss_box_reg: 0.5399  loss_rpn_cls: 0.05065  loss_rpn_loc: 0.1825  loss_image_d: 0.08658  loss_instance_d: 0.00762     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:29:41 d2.utils.events]:  eta: 1:43:41  iter: 199  total_loss: 1.282  loss_cls: 0.366  loss_box_reg: 0.4881  loss_rpn_cls: 0.05973  loss_rpn_loc: 0.181  loss_image_d: 0.08657  loss_instance_d: 0.005387     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:31:34 d2.utils.events]:  eta: 1:43:29  iter: 249  total_loss: 1.144  loss_cls: 0.3488  loss_box_reg: 0.5163  loss_rpn_cls: 0.05088  loss_rpn_loc: 0.1712  loss_image_d: 0.08656  loss_instance_d: 0.002864     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:33:26 d2.utils.events]:  eta: 1:41:11  iter: 299  total_loss: 1.229  loss_cls: 0.3498  loss_box_reg: 0.5058  loss_rpn_cls: 0.05671  loss_rpn_loc: 0.1749  loss_image_d: 0.08655  loss_instance_d: 0.002274     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:35:15 d2.utils.events]:  eta: 1:36:33  iter: 349  total_loss: 1.113  loss_cls: 0.3333  loss_box_reg: 0.4588  loss_rpn_cls: 0.04668  loss_rpn_loc: 0.1847  loss_image_d: 0.08654  loss_instance_d: 0.001827     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:37:08 d2.utils.events]:  eta: 1:37:33  iter: 399  total_loss: 1.213  loss_cls: 0.3345  loss_box_reg: 0.5069  loss_rpn_cls: 0.05122  loss_rpn_loc: 0.1976  loss_image_d: 0.08653  loss_instance_d: 0.00131     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:39:00 d2.utils.events]:  eta: 1:35:26  iter: 449  total_loss: 1.222  loss_cls: 0.3588  loss_box_reg: 0.4957  loss_rpn_cls: 0.0536  loss_rpn_loc: 0.195  loss_image_d: 0.08652  loss_instance_d: 0.001146     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:40:48 d2.utils.events]:  eta: 1:30:07  iter: 499  total_loss: 1.229  loss_cls: 0.3672  loss_box_reg: 0.5057  loss_rpn_cls: 0.04809  loss_rpn_loc: 0.1989  loss_image_d: 0.08651  loss_instance_d: 0.001043     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:42:37 d2.utils.events]:  eta: 1:28:57  iter: 549  total_loss: 1.178  loss_cls: 0.3399  loss_box_reg: 0.4961  loss_rpn_cls: 0.04655  loss_rpn_loc: 0.2069  loss_image_d: 0.0865  loss_instance_d: 0.0008645     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:44:24 d2.utils.events]:  eta: 1:25:07  iter: 599  total_loss: 1.012  loss_cls: 0.307  loss_box_reg: 0.4324  loss_rpn_cls: 0.04075  loss_rpn_loc: 0.1594  loss_image_d: 0.08649  loss_instance_d: 0.000751     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:46:11 d2.utils.events]:  eta: 1:23:46  iter: 649  total_loss: 1.16  loss_cls: 0.3271  loss_box_reg: 0.4774  loss_rpn_cls: 0.04022  loss_rpn_loc: 0.1994  loss_image_d: 0.08647  loss_instance_d: 0.0007259     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:48:00 d2.utils.events]:  eta: 1:24:09  iter: 699  total_loss: 1.108  loss_cls: 0.3052  loss_box_reg: 0.4342  loss_rpn_cls: 0.04473  loss_rpn_loc: 0.1772  loss_image_d: 0.08645  loss_instance_d: 0.0005661     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:49:46 d2.utils.events]:  eta: 1:19:17  iter: 749  total_loss: 1.184  loss_cls: 0.3162  loss_box_reg: 0.4888  loss_rpn_cls: 0.04791  loss_rpn_loc: 0.2016  loss_image_d: 0.08645  loss_instance_d: 0.0004795     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:51:19 d2.utils.events]:  eta: 1:08:13  iter: 799  total_loss: 1.098  loss_cls: 0.3339  loss_box_reg: 0.455  loss_rpn_cls: 0.0439  loss_rpn_loc: 0.197  loss_image_d: 0.08643  loss_instance_d: 0.0004913     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:52:54 d2.utils.events]:  eta: 1:08:19  iter: 849  total_loss: 1.06  loss_cls: 0.3022  loss_box_reg: 0.4648  loss_rpn_cls: 0.04615  loss_rpn_loc: 0.1681  loss_image_d: 0.08642  loss_instance_d: 0.0004776     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:54:29 d2.utils.events]:  eta: 1:06:15  iter: 899  total_loss: 1.091  loss_cls: 0.3187  loss_box_reg: 0.4663  loss_rpn_cls: 0.05128  loss_rpn_loc: 0.1789  loss_image_d: 0.0864  loss_instance_d: 0.0003861     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:56:03 d2.utils.events]:  eta: 1:04:06  iter: 949  total_loss: 1.116  loss_cls: 0.3141  loss_box_reg: 0.466  loss_rpn_cls: 0.04265  loss_rpn_loc: 0.1742  loss_image_d: 0.08638  loss_instance_d: 0.0003674     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:57:38 d2.utils.events]:  eta: 1:03:35  iter: 999  total_loss: 1.04  loss_cls: 0.2748  loss_box_reg: 0.4215  loss_rpn_cls: 0.04076  loss_rpn_loc: 0.186  loss_image_d: 0.08636  loss_instance_d: 0.000352     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 21:59:13 d2.utils.events]:  eta: 1:01:23  iter: 1049  total_loss: 1.126  loss_cls: 0.3077  loss_box_reg: 0.4509  loss_rpn_cls: 0.0483  loss_rpn_loc: 0.2096  loss_image_d: 0.08635  loss_instance_d: 0.0003097     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:00:48 d2.utils.events]:  eta: 1:00:09  iter: 1099  total_loss: 1.138  loss_cls: 0.3173  loss_box_reg: 0.4601  loss_rpn_cls: 0.05331  loss_rpn_loc: 0.1938  loss_image_d: 0.08632  loss_instance_d: 0.0003115     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:02:23 d2.utils.events]:  eta: 0:58:30  iter: 1149  total_loss: 1.124  loss_cls: 0.3247  loss_box_reg: 0.4662  loss_rpn_cls: 0.05683  loss_rpn_loc: 0.2066  loss_image_d: 0.0863  loss_instance_d: 0.0003052     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:03:57 d2.utils.events]:  eta: 0:56:28  iter: 1199  total_loss: 1.072  loss_cls: 0.3003  loss_box_reg: 0.4512  loss_rpn_cls: 0.04476  loss_rpn_loc: 0.1601  loss_image_d: 0.08628  loss_instance_d: 0.0002756     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:05:31 d2.utils.events]:  eta: 0:54:57  iter: 1249  total_loss: 1.122  loss_cls: 0.3224  loss_box_reg: 0.4696  loss_rpn_cls: 0.04472  loss_rpn_loc: 0.2097  loss_image_d: 0.08625  loss_instance_d: 0.0002589     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:07:06 d2.utils.events]:  eta: 0:53:55  iter: 1299  total_loss: 1.109  loss_cls: 0.3185  loss_box_reg: 0.4534  loss_rpn_cls: 0.04829  loss_rpn_loc: 0.1828  loss_image_d: 0.08622  loss_instance_d: 0.0002312     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:08:40 d2.utils.events]:  eta: 0:51:39  iter: 1349  total_loss: 0.9899  loss_cls: 0.2872  loss_box_reg: 0.4174  loss_rpn_cls: 0.04299  loss_rpn_loc: 0.1619  loss_image_d: 0.0862  loss_instance_d: 0.0002171     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:10:16 d2.utils.events]:  eta: 0:51:17  iter: 1399  total_loss: 1.08  loss_cls: 0.2992  loss_box_reg: 0.4542  loss_rpn_cls: 0.04137  loss_rpn_loc: 0.1966  loss_image_d: 0.08616  loss_instance_d: 0.0002255     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:11:52 d2.utils.events]:  eta: 0:49:26  iter: 1449  total_loss: 1.081  loss_cls: 0.3062  loss_box_reg: 0.4537  loss_rpn_cls: 0.04404  loss_rpn_loc: 0.2145  loss_image_d: 0.08614  loss_instance_d: 0.0002507     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:13:25 d2.utils.events]:  eta: 0:46:46  iter: 1499  total_loss: 1.1  loss_cls: 0.2864  loss_box_reg: 0.4489  loss_rpn_cls: 0.04577  loss_rpn_loc: 0.2163  loss_image_d: 0.08611  loss_instance_d: 0.0002492     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:15:00 d2.utils.events]:  eta: 0:45:46  iter: 1549  total_loss: 1.006  loss_cls: 0.2863  loss_box_reg: 0.4474  loss_rpn_cls: 0.04261  loss_rpn_loc: 0.1496  loss_image_d: 0.08607  loss_instance_d: 0.0002089     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:16:34 d2.utils.events]:  eta: 0:43:54  iter: 1599  total_loss: 1.028  loss_cls: 0.3065  loss_box_reg: 0.4322  loss_rpn_cls: 0.03895  loss_rpn_loc: 0.1835  loss_image_d: 0.08604  loss_instance_d: 0.0001983     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:18:09 d2.utils.events]:  eta: 0:42:46  iter: 1649  total_loss: 1.089  loss_cls: 0.2977  loss_box_reg: 0.4398  loss_rpn_cls: 0.04713  loss_rpn_loc: 0.1995  loss_image_d: 0.08601  loss_instance_d: 0.0001819     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:19:45 d2.utils.events]:  eta: 0:41:22  iter: 1699  total_loss: 1.07  loss_cls: 0.2905  loss_box_reg: 0.4503  loss_rpn_cls: 0.04776  loss_rpn_loc: 0.2167  loss_image_d: 0.08596  loss_instance_d: 0.0002026     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:21:19 d2.utils.events]:  eta: 0:39:21  iter: 1749  total_loss: 1.061  loss_cls: 0.2897  loss_box_reg: 0.438  loss_rpn_cls: 0.04283  loss_rpn_loc: 0.1751  loss_image_d: 0.08591  loss_instance_d: 0.0001927     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:22:53 d2.utils.events]:  eta: 0:37:32  iter: 1799  total_loss: 1.165  loss_cls: 0.3173  loss_box_reg: 0.456  loss_rpn_cls: 0.04269  loss_rpn_loc: 0.1874  loss_image_d: 0.08585  loss_instance_d: 0.0001961     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:24:28 d2.utils.events]:  eta: 0:36:31  iter: 1849  total_loss: 1.09  loss_cls: 0.3132  loss_box_reg: 0.4355  loss_rpn_cls: 0.03889  loss_rpn_loc: 0.184  loss_image_d: 0.0858  loss_instance_d: 0.0002036     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:26:03 d2.utils.events]:  eta: 0:34:41  iter: 1899  total_loss: 1.11  loss_cls: 0.2946  loss_box_reg: 0.4553  loss_rpn_cls: 0.04676  loss_rpn_loc: 0.2188  loss_image_d: 0.08573  loss_instance_d: 0.0001856     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:27:38 d2.utils.events]:  eta: 0:33:09  iter: 1949  total_loss: 1.073  loss_cls: 0.2916  loss_box_reg: 0.4663  loss_rpn_cls: 0.04163  loss_rpn_loc: 0.1995  loss_image_d: 0.08568  loss_instance_d: 0.0001727     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:29:13 d2.utils.events]:  eta: 0:31:46  iter: 1999  total_loss: 1.041  loss_cls: 0.3091  loss_box_reg: 0.4574  loss_rpn_cls: 0.04622  loss_rpn_loc: 0.1598  loss_image_d: 0.08559  loss_instance_d: 0.0001657     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:30:47 d2.utils.events]:  eta: 0:29:43  iter: 2049  total_loss: 1.048  loss_cls: 0.2786  loss_box_reg: 0.4555  loss_rpn_cls: 0.04037  loss_rpn_loc: 0.1804  loss_image_d: 0.08554  loss_instance_d: 0.0001778     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:32:21 d2.utils.events]:  eta: 0:28:19  iter: 2099  total_loss: 1.045  loss_cls: 0.2973  loss_box_reg: 0.4165  loss_rpn_cls: 0.0424  loss_rpn_loc: 0.1792  loss_image_d: 0.08544  loss_instance_d: 0.0001656     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:33:57 d2.utils.events]:  eta: 0:27:04  iter: 2149  total_loss: 1.136  loss_cls: 0.3342  loss_box_reg: 0.474  loss_rpn_cls: 0.03619  loss_rpn_loc: 0.1906  loss_image_d: 0.08535  loss_instance_d: 0.0001692     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:35:32 d2.utils.events]:  eta: 0:25:17  iter: 2199  total_loss: 1.031  loss_cls: 0.2878  loss_box_reg: 0.4297  loss_rpn_cls: 0.03778  loss_rpn_loc: 0.1598  loss_image_d: 0.08525  loss_instance_d: 0.0001808     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:37:06 d2.utils.events]:  eta: 0:23:35  iter: 2249  total_loss: 1.046  loss_cls: 0.2851  loss_box_reg: 0.4587  loss_rpn_cls: 0.04243  loss_rpn_loc: 0.2161  loss_image_d: 0.08516  loss_instance_d: 0.0001521     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:38:42 d2.utils.events]:  eta: 0:22:14  iter: 2299  total_loss: 1.025  loss_cls: 0.2778  loss_box_reg: 0.4252  loss_rpn_cls: 0.03494  loss_rpn_loc: 0.1818  loss_image_d: 0.08504  loss_instance_d: 0.0001657     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:40:16 d2.utils.events]:  eta: 0:20:23  iter: 2349  total_loss: 0.9758  loss_cls: 0.2631  loss_box_reg: 0.413  loss_rpn_cls: 0.04059  loss_rpn_loc: 0.1614  loss_image_d: 0.08494  loss_instance_d: 0.0001495     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:41:49 d2.utils.events]:  eta: 0:18:46  iter: 2399  total_loss: 1.033  loss_cls: 0.2876  loss_box_reg: 0.4273  loss_rpn_cls: 0.04894  loss_rpn_loc: 0.1744  loss_image_d: 0.08482  loss_instance_d: 0.000157     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:43:25 d2.utils.events]:  eta: 0:17:25  iter: 2449  total_loss: 1.041  loss_cls: 0.299  loss_box_reg: 0.4356  loss_rpn_cls: 0.04135  loss_rpn_loc: 0.1662  loss_image_d: 0.08463  loss_instance_d: 0.0001481     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:44:59 d2.utils.events]:  eta: 0:15:42  iter: 2499  total_loss: 1.059  loss_cls: 0.2939  loss_box_reg: 0.4383  loss_rpn_cls: 0.04407  loss_rpn_loc: 0.1792  loss_image_d: 0.08453  loss_instance_d: 0.0001426     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:46:33 d2.utils.events]:  eta: 0:14:09  iter: 2549  total_loss: 1.023  loss_cls: 0.3024  loss_box_reg: 0.449  loss_rpn_cls: 0.03723  loss_rpn_loc: 0.1595  loss_image_d: 0.08431  loss_instance_d: 0.0001238     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:48:09 d2.utils.events]:  eta: 0:12:43  iter: 2599  total_loss: 1.083  loss_cls: 0.2982  loss_box_reg: 0.4378  loss_rpn_cls: 0.03861  loss_rpn_loc: 0.2342  loss_image_d: 0.08412  loss_instance_d: 0.0001253     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:49:43 d2.utils.events]:  eta: 0:10:59  iter: 2649  total_loss: 0.928  loss_cls: 0.2631  loss_box_reg: 0.3906  loss_rpn_cls: 0.04246  loss_rpn_loc: 0.171  loss_image_d: 0.08392  loss_instance_d: 0.0001393     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:51:18 d2.utils.events]:  eta: 0:09:31  iter: 2699  total_loss: 1.06  loss_cls: 0.3072  loss_box_reg: 0.437  loss_rpn_cls: 0.04349  loss_rpn_loc: 0.1991  loss_image_d: 0.08376  loss_instance_d: 0.0001416     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:52:53 d2.utils.events]:  eta: 0:07:52  iter: 2749  total_loss: 1.015  loss_cls: 0.2654  loss_box_reg: 0.389  loss_rpn_cls: 0.04161  loss_rpn_loc: 0.1824  loss_image_d: 0.08342  loss_instance_d: 0.0001293     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:54:26 d2.utils.events]:  eta: 0:06:14  iter: 2799  total_loss: 1.089  loss_cls: 0.3083  loss_box_reg: 0.4641  loss_rpn_cls: 0.03976  loss_rpn_loc: 0.191  loss_image_d: 0.08319  loss_instance_d: 0.0001355     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:56:02 d2.utils.events]:  eta: 0:04:46  iter: 2849  total_loss: 1.126  loss_cls: 0.2911  loss_box_reg: 0.451  loss_rpn_cls: 0.02985  loss_rpn_loc: 0.2095  loss_image_d: 0.08289  loss_instance_d: 0.0001266     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:57:36 d2.utils.events]:  eta: 0:03:08  iter: 2899  total_loss: 1.021  loss_cls: 0.2704  loss_box_reg: 0.4293  loss_rpn_cls: 0.04085  loss_rpn_loc: 0.1644  loss_image_d: 0.08278  loss_instance_d: 0.0001295     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 22:59:10 d2.utils.events]:  eta: 0:01:34  iter: 2949  total_loss: 0.9628  loss_cls: 0.2756  loss_box_reg: 0.4223  loss_rpn_cls: 0.03763  loss_rpn_loc: 0.1689  loss_image_d: 0.08238  loss_instance_d: 0.0001193     lr: 0.0005  max_mem: 5695M\n",
            "[09/17 23:00:45 d2.utils.events]:  eta: 0:00:00  iter: 2999  total_loss: 0.9958  loss_cls: 0.2519  loss_box_reg: 0.4175  loss_rpn_cls: 0.03311  loss_rpn_loc: 0.2036  loss_image_d: 0.08213  loss_instance_d: 0.0001186     lr: 0.0005  max_mem: 5695M\n"
          ]
        }
      ],
      "source": [
        "do_train(cfg_source,cfg_target,model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M1rLpXVddJC"
      },
      "source": [
        "##Evalutate the performance\n",
        "runt the PascalVOCDetectionEvaluator if your annotations are in PASCAL VOC otherwhise run the COCOEvaluator<br>\n",
        "\n",
        "The mAP50 is the object detection result on the dataset. In this case, for the cityscape dataset, the result is 39.5%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xInwPrjxdngu",
        "outputId": "6c566a3e-9390-4607-a60c-7fe87ebbde10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[09/17 23:00:48 d2.data.build]: Distribution of instances among all 8 categories:\n",
            "|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|    car     | 4667         |   person   | 3419         |   rider    | 556          |\n",
            "|   truck    | 93           |    bus     | 98           |   train    | 23           |\n",
            "| motorcycle | 149          |  bicycle   | 1175         |            |              |\n",
            "|   total    | 10180        |            |              |            |              |\n",
            "[09/17 23:00:48 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(0, 0), max_size=1333, sample_style='choice')]\n",
            "[09/17 23:00:48 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[09/17 23:00:48 d2.data.common]: Serializing 500 elements to byte tensors and concatenating them all ...\n",
            "[09/17 23:00:48 d2.data.common]: Serialized dataset takes 0.80 MiB\n",
            "[09/17 23:00:48 d2.evaluation.evaluator]: Start inference on 500 batches\n",
            "[09/17 23:00:51 d2.evaluation.evaluator]: Inference done 11/500. Dataloading: 0.0027 s/iter. Inference: 0.2162 s/iter. Eval: 0.0008 s/iter. Total: 0.2197 s/iter. ETA=0:01:47\n",
            "[09/17 23:00:56 d2.evaluation.evaluator]: Inference done 34/500. Dataloading: 0.0031 s/iter. Inference: 0.2175 s/iter. Eval: 0.0008 s/iter. Total: 0.2215 s/iter. ETA=0:01:43\n",
            "[09/17 23:01:01 d2.evaluation.evaluator]: Inference done 56/500. Dataloading: 0.0059 s/iter. Inference: 0.2173 s/iter. Eval: 0.0010 s/iter. Total: 0.2243 s/iter. ETA=0:01:39\n",
            "[09/17 23:01:07 d2.evaluation.evaluator]: Inference done 79/500. Dataloading: 0.0053 s/iter. Inference: 0.2177 s/iter. Eval: 0.0009 s/iter. Total: 0.2240 s/iter. ETA=0:01:34\n",
            "[09/17 23:01:12 d2.evaluation.evaluator]: Inference done 102/500. Dataloading: 0.0051 s/iter. Inference: 0.2180 s/iter. Eval: 0.0009 s/iter. Total: 0.2241 s/iter. ETA=0:01:29\n",
            "[09/17 23:01:17 d2.evaluation.evaluator]: Inference done 124/500. Dataloading: 0.0057 s/iter. Inference: 0.2181 s/iter. Eval: 0.0009 s/iter. Total: 0.2249 s/iter. ETA=0:01:24\n",
            "[09/17 23:01:22 d2.evaluation.evaluator]: Inference done 147/500. Dataloading: 0.0053 s/iter. Inference: 0.2181 s/iter. Eval: 0.0008 s/iter. Total: 0.2245 s/iter. ETA=0:01:19\n",
            "[09/17 23:01:27 d2.evaluation.evaluator]: Inference done 170/500. Dataloading: 0.0055 s/iter. Inference: 0.2182 s/iter. Eval: 0.0008 s/iter. Total: 0.2247 s/iter. ETA=0:01:14\n",
            "[09/17 23:01:32 d2.evaluation.evaluator]: Inference done 193/500. Dataloading: 0.0059 s/iter. Inference: 0.2180 s/iter. Eval: 0.0008 s/iter. Total: 0.2249 s/iter. ETA=0:01:09\n",
            "[09/17 23:01:37 d2.evaluation.evaluator]: Inference done 216/500. Dataloading: 0.0057 s/iter. Inference: 0.2180 s/iter. Eval: 0.0008 s/iter. Total: 0.2246 s/iter. ETA=0:01:03\n",
            "[09/17 23:01:42 d2.evaluation.evaluator]: Inference done 239/500. Dataloading: 0.0057 s/iter. Inference: 0.2179 s/iter. Eval: 0.0007 s/iter. Total: 0.2245 s/iter. ETA=0:00:58\n",
            "[09/17 23:01:48 d2.evaluation.evaluator]: Inference done 262/500. Dataloading: 0.0057 s/iter. Inference: 0.2177 s/iter. Eval: 0.0008 s/iter. Total: 0.2243 s/iter. ETA=0:00:53\n",
            "[09/17 23:01:53 d2.evaluation.evaluator]: Inference done 285/500. Dataloading: 0.0055 s/iter. Inference: 0.2176 s/iter. Eval: 0.0008 s/iter. Total: 0.2240 s/iter. ETA=0:00:48\n",
            "[09/17 23:01:58 d2.evaluation.evaluator]: Inference done 308/500. Dataloading: 0.0057 s/iter. Inference: 0.2174 s/iter. Eval: 0.0008 s/iter. Total: 0.2239 s/iter. ETA=0:00:42\n",
            "[09/17 23:02:03 d2.evaluation.evaluator]: Inference done 331/500. Dataloading: 0.0056 s/iter. Inference: 0.2173 s/iter. Eval: 0.0008 s/iter. Total: 0.2239 s/iter. ETA=0:00:37\n",
            "[09/17 23:02:08 d2.evaluation.evaluator]: Inference done 354/500. Dataloading: 0.0055 s/iter. Inference: 0.2172 s/iter. Eval: 0.0008 s/iter. Total: 0.2236 s/iter. ETA=0:00:32\n",
            "[09/17 23:02:13 d2.evaluation.evaluator]: Inference done 377/500. Dataloading: 0.0056 s/iter. Inference: 0.2171 s/iter. Eval: 0.0008 s/iter. Total: 0.2236 s/iter. ETA=0:00:27\n",
            "[09/17 23:02:18 d2.evaluation.evaluator]: Inference done 400/500. Dataloading: 0.0055 s/iter. Inference: 0.2170 s/iter. Eval: 0.0008 s/iter. Total: 0.2235 s/iter. ETA=0:00:22\n",
            "[09/17 23:02:23 d2.evaluation.evaluator]: Inference done 423/500. Dataloading: 0.0054 s/iter. Inference: 0.2170 s/iter. Eval: 0.0008 s/iter. Total: 0.2233 s/iter. ETA=0:00:17\n",
            "[09/17 23:02:28 d2.evaluation.evaluator]: Inference done 446/500. Dataloading: 0.0054 s/iter. Inference: 0.2170 s/iter. Eval: 0.0008 s/iter. Total: 0.2234 s/iter. ETA=0:00:12\n",
            "[09/17 23:02:34 d2.evaluation.evaluator]: Inference done 469/500. Dataloading: 0.0053 s/iter. Inference: 0.2171 s/iter. Eval: 0.0008 s/iter. Total: 0.2233 s/iter. ETA=0:00:06\n",
            "[09/17 23:02:39 d2.evaluation.evaluator]: Inference done 492/500. Dataloading: 0.0052 s/iter. Inference: 0.2171 s/iter. Eval: 0.0008 s/iter. Total: 0.2233 s/iter. ETA=0:00:01\n",
            "[09/17 23:02:41 d2.evaluation.evaluator]: Total inference time: 0:01:50.616504 (0.223468 s / iter per device, on 1 devices)\n",
            "[09/17 23:02:41 d2.evaluation.evaluator]: Total inference pure compute time: 0:01:47 (0.217135 s / iter per device, on 1 devices)\n",
            "[09/17 23:02:41 d2.evaluation.pascal_voc_evaluation]: Evaluating city_testT using 2007 metric. Note that results do not use the official Matlab API.\n",
            "OrderedDict([('bbox', {'AP': 23.37023422827704, 'AP50': 39.52720214346048, 'AP75': 24.16825878188736})])\n"
          ]
        }
      ],
      "source": [
        "#PASCAL VOC evaluation\n",
        "from detectron2.evaluation import inference_on_dataset, PascalVOCDetectionEvaluator\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = PascalVOCDetectionEvaluator(\"city_testT\")\n",
        "val_loader = build_detection_test_loader(cfg_source, \"city_testT\")\n",
        "res = inference_on_dataset(model, val_loader, evaluator)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNnUC4OCd40W"
      },
      "outputs": [],
      "source": [
        "#COCO evaluation example\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "evaluator = COCOEvaluator(\"dataset_test_real\", cfg_source, False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg_source, \"dataset_test_real\")\n",
        "inference_on_dataset(model, val_loader, evaluator)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
